% \VignetteIndexEntry{lazy.frame Manual}
% \VignetteDepends{lazy.frame}
% \VignettePackage{lazy.frame}
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage[
     colorlinks=true,
     linkcolor=blue,
     citecolor=blue,
     urlcolor=blue]
     {hyperref}
\usepackage{lscape}
\usepackage{Sweave}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{mdwlist}

\usepackage{lmodern}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bld}[1]{\mbox{\boldmath $#1$}}
\newcommand{\shell}[1]{\mbox{$#1$}}
\renewcommand{\vec}[1]{\mbox{\bf {#1}}}
\newcommand{\ReallySmallSpacing}{\renewcommand{\baselinestretch}{.6}\Large\normalsize}
\newcommand{\SmallSpacing}{\renewcommand{\baselinestretch}{1.1}\Large\normalsize}
\def\tm{\leavevmode\hbox{$\rm {}^{TM}$}}


\setlength{\oddsidemargin}{-.25 truein}
\setlength{\evensidemargin}{0truein}
\setlength{\topmargin}{-0.2truein}
\setlength{\textwidth}{7 truein}
\setlength{\textheight}{8.5 truein}
\setlength{\parindent}{0.20truein}
\setlength{\parskip}{0.10truein}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\lhead{}
\chead{The {\tt lazy.frame} Package}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{The {\tt lazy.frame} Package}
\author{Bryan W. Lewis \\ 
blewis@illposed.net}

\begin{document}

\maketitle

\thispagestyle{empty}

\section{Preface}

I've been working with some large-ish text files of comma separated values
(CSV) recently. The files are each about two gigabytes with about 20 million
rows. My computer has plenty of memory for R to load each file.

\noindent But, it takes a while.

\noindent And I'm impatient.

\noindent Now, I don't really need the entire data set in memory. I really just
need to filter the data a bit and then sample from the rows. I think that this
situation is typical enough--wanting fast access to subsets of large text
files--that I wrote this package for it.

The {\tt lazy.frame} package lets me quickly and efficiently work with subsets
from a text file without loading the entire file into memory. A ``lazy.frame''
presents a text file as a kind of simple data frame, but without first loading
the file into memory. Lazy frames lazily load data from their backing files
only when required, for example by an indexing operation. They are essentially
lazy wrappers for the {\tt read.table} function with a few extra convenience
functions.

There are several compelling R packages for working directly with file-backed
data: The
\href{http://cran.r-project.org/web/packages/bigmemory/index.html}{bigmemory}
package by Emerson and Kane provides a memory mapped matrix object, free from R
indexing constraints, and a comprehensive suite of fast analysis
functions.  The nicely simple but powerful
\href{http://cran.r-project.org/web/packages/mmap/index.html}{mmap} package by
Jeff Ryan defines a data frame-like memory mapped object. And the venerable
\href{http://cran.r-project.org/web/packages/ff/index.html}{ff} package by
Adler, Oehlschl\"agel, et. al. defines a variety of memory mapped data
frame-like objects and functions. All of these packages have really interesting
features. Most of them are designed to facilitate working with objects larger
than the physical RAM available on a computer.

But, recall that my data sets fit into the RAM on my computer (RAM is really
cheap)! My main irritation is the bottleneck incurred by parsing the entire
data set, which isn't really avoided by the above packages (although some of
the packages do include methods to help expedite loading data from text files).

Of course, lazy frames aren't a panacea and have limitations discussed below.
If you need to compute with all of the data in a file, then bite the bullet and
load the whole file, or consider using {\tt ff}, {\tt mmap}, or {\tt bigmemory}
if you're short of RAM.  For \emph{really} large data sets, or for more
sophisticated operations involving all the data, {\tt bigmemory} is a better
option.  Lazy frames are really good for quickly extracting subsets from large
text files with between roughly a million and a hundred million or so rows.

\section{Using {\tt lazy.frame} package} \section{Limitations}
\section{Examples} I present a few examples that compare indexing operations on
lazy frames with indexing operations on data frames read in by {\tt
read.table}. All experiments were conducted on a 2\,GHz, four-core AMD Opetron
computer with 12\,GB of DDR-2 RAM running Ubuntu 9.10 GNU/Linux and R version
2.12.1. The data files resided on a Fusio-io ioXtreme solid state disk rated at
700MB/s data read rate and 80$\mu$s read latency. In order to minimize disk
caching effects between tests, the command
\begin{lstlisting}
echo 3 > /proc/sys/vm/drop_caches
\end{lstlisting}
was issued (wiping clean the Linux disk memory cache) just before each test.

\newpage
\subsection{Uncompressed file examples}
I used {\tt read.table} with and without defining column classes to read the
data into a data frame from an uncompressed file. As expected, specifying
column classes in {\tt read.table}
reduced the load time by more than 20\% in this example, and greatly
reduced the maxmimum memory consumption during loading 
from almost 8\,GB to under 5\,GB. Without column classes, it took
over 11 minutes to load the data in. Specifying column classes reduced
that to about 9 minutes.

Once loaded, I extracted a subset of
about 95 thousand rows in which the 20th column had values greater than
zero. It took about 27 seconds to extract the subset.

Lazy frame took only about 4 seconds to ``load'' the same file, and about 53
seconds to extract the same row subset. Thus, we see the penalty of lazily
loading data from the file--it took about twice as long to extract the subset
in this example. But, we avoided the substantial initial load time almost
completely. And, the maximum memory used by the R session was limited to about
the 18MB memory required to hold the subset.

\lstset{
  morecomment=[l][\textbf]{  use},
  morecomment=[l][\textbf]{[1]},
  morecomment=[l][\textbf]{648},
  morecomment=[l][\textbf]{443},
  morecomment=[l][\textbf]{ 2},
  morecomment=[l][\textbf]{ 40},
  morecomment=[l][\textbf]{Ncel},
  morecomment=[l][\textbf]{Vcel},
  frame=single,
  basicstyle=\small,
  breaklines=true,
}

\lstset{caption=Extract a subset from a lazy frame.}
\begin{lstlisting}
library("lazy.frame")

t1 = proc.time()
x = file.frame(file="test.csv")
print(proc.time() - t1)
   user  system elapsed
   2.34    2.05    4.39

print(gc())
         used (Mb) gc trigger (Mb) max used (Mb)
Ncells 140517  7.6     350000 18.7   350000 18.7
Vcells 130910  1.0     786432  6.0   531925  4.1

print(dim(x))
[1] 17826159       27

t1 = proc.time()
z = x[x[,20]>0,]
print(proc.time() - t1)
   user  system elapsed
 40.870  11.770  52.709

print(dim(z))
[1] 95166    27
\end{lstlisting}

\newpage
\lstset{caption=Extract a subset from a data frame loaded with {\tt read.table}}
\begin{lstlisting}
t1 = proc.time()
x = read.table(file="test.csv",header=FALSE,sep=",",stringsAsFactors=FALSE)
print(proc.time() - t1)
   user  system elapsed
648.380  33.350 682.699

print(gc())
            used   (Mb) gc trigger   (Mb)   max used   (Mb)
Ncells    138089    7.4     667722   35.7     380666   20.4
Vcells 285413776 2177.6  832606162 6352.3 1034548528 7893.0

print(dim(x))
[1] 17826159       27

t1 = proc.time()
y = x[x[,20]>0,]
print(proc.time() - t1)
  user  system elapsed
 27.87    2.41   30.31

print(dim(y))
[1] 95166    27
\end{lstlisting}


\newpage
\lstset{caption=Extract a subset from a data frame loaded with {\tt read.table}
with defined column classes.}
\begin{lstlisting}
cc = c("numeric","integer","integer","integer","integer",
       "integer","integer","integer","integer","character",
       "character","integer","integer","integer","integer",
       "integer","integer","integer","integer","integer",
       "integer","integer","integer","numeric","integer",
       "numeric","integer")
t1 = proc.time()
x = read.table(file="test.csv",header=FALSE,sep=",",stringsAsFactors=FALSE, colClasses=cc)
print(proc.time() - t1)
   user  system elapsed
443.290  82.780 526.141

print(gc())
            used   (Mb) gc trigger   (Mb)  max used   (Mb)
Ncells    138519    7.4     350000   18.7    350000   18.7
Vcells 285348278 2177.1  649037152 4951.8 641872298 4897.1

print(dim(x))
[1] 17826159       27

t1 = proc.time()
y = x[x[,20]>0,]
print(proc.time() - t1)
   user  system elapsed
 28.410   2.180  30.593

print(dim(y))
[1] 95166    27
\end{lstlisting}

\end{document}
