% \VignetteIndexEntry{file.frame Manual}
% \VignetteDepends{file.frame}
% \VignettePackage{file.frame}
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage[
     colorlinks=true,
     linkcolor=blue,
     citecolor=blue,
     urlcolor=blue]
     {hyperref}
\usepackage{lscape}
\usepackage{Sweave}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{mdwlist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bld}[1]{\mbox{\boldmath $#1$}}
\newcommand{\shell}[1]{\mbox{$#1$}}
\renewcommand{\vec}[1]{\mbox{\bf {#1}}}
\newcommand{\ReallySmallSpacing}{\renewcommand{\baselinestretch}{.6}\Large\normalsize}
\newcommand{\SmallSpacing}{\renewcommand{\baselinestretch}{1.1}\Large\normalsize}
\def\tm{\leavevmode\hbox{$\rm {}^{TM}$}}


\setlength{\oddsidemargin}{-.25 truein}
\setlength{\evensidemargin}{0truein}
\setlength{\topmargin}{-0.2truein}
\setlength{\textwidth}{7 truein}
\setlength{\textheight}{8.5 truein}
\setlength{\parindent}{0.20truein}
\setlength{\parskip}{0.10truein}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\lhead{}
\chead{The {\tt file.frame} Package}
\rhead{}
\lfoot{}
\cfoot{}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{The {\tt file.frame} Package}
\author{Bryan W. Lewis \\ 
blewis@illposed.net}

\begin{document}

\maketitle

\thispagestyle{empty}

\section{Preface}

I've been working with some large-ish text files of comma separated values
(CSV).  The files are each about three gigabytes with about 20 million rows. My
computer has enough memory for R to load the data.

\noindent But it takes a while.

\noindent And I'm impatient.

\noindent Now, I don't really need the entire data set in memory. I really just
need to filter the data a bit and then sample from the rows. I think this
situation is typical enough--wanting fast access to subsets of large text
files--that I wrote this package for it.

The {\tt file.frame} package lets me quickly and efficiently work with subsets
from a text file without loading the entire file into memory. A ``file frame''
presents a text file as a kind of simple data frame, but without first loading
the file into memory. File frames lazily load data from their backing files
only when required, for example by an indexing operation. They are essentially
a lazy wrapper for the {\tt read.table} function with a few extra convenience
features.

There are several compelling R packages for working directly with file-backed
data: The
\href{http://cran.r-project.org/web/packages/bigmemory/index.html}{bigmemory}
package by Emmerson and Kane provides a memory mapped matrix object free of R
indexing constraints along with a comprehensive suite of fast analysis
functions.  The straightforward to use and powerful
\href{http://cran.r-project.org/web/packages/mmap/index.html}{mmap} package by
Jeff Ryan defines a data frame-like memory mapped object. And the venerable
\href{http://cran.r-project.org/web/packages/ff/index.html}{ff} package by
Adler, Oehlschl\"agel, et. al. defines a variety of memory mapped data
frame-like objects and functions. All of these packages have really interesting
features. Most of them are designed to facilitate working with objects larger
than the physical RAM available on a computer.

But, my data sets fit into the RAM on my computer (RAM is really cheap)! My
main problem is the bottleneck from reading the entire data set, which isn't
avoided by the above packages (although some of the packages do include
methods to help expedite loading data from text files).

File frames aren't a panacea and have limitations discussed below. They are
good for quickly extracting subsets  from raw text files. They are well-suited
to medium-sized data files with millions or tens of millions of rows. For
\emph{really} large data sets, {\tt bigmemory} is a good option.

\section{Using {\tt file.frame} package}
\section{Limitations}
\section{Examples}



\end{document}
